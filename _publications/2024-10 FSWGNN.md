---
title: "FSW-GNN: A Bi-Lipschitz WL-Equivalent Graph Neural Network"
collection: publications
permalink: "/publication/2024-10 FSWGNN"
venue: '*Learning on Graphs* (LoG 2025)'
authors: "Yonatan Sverdlov, Yair Davidson, Nadav Dym, Tal Amir"
paperurl: 'https://arxiv.org/abs/2410.09118'
excerpt: #'A short description'
#date: #2023-12-01
slidesurl: #'http://academicpages.github.io/files/slides2.pdf'
citation: #'Your Name, You. (2024). &quot;Paper Title Number 3.&quot; <i>GitHub Journal of Bugs</i>. 1(3).'
#abstract: 'This is a test abstract *good one* **nice one**'
bibtex: |
    ```
    @misc{sverdlov2024fswgnn,
    title={{FSW-GNN}: {A} Bi-{L}ipschitz {WL}-Equivalent Graph Neural Network},
    author={Sverdlov, Yonatan and Davidson, Yair and Dym, Nadav and Amir, Tal},
    year={2024},
    eprint={2410.09118},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2410.09118}
    }
    ```
---


**Abstract**

Many of the most popular graph neural networks fall into the category of Message Passing Neural Networks (MPNNs). Famously,  MPNNs ability to distinguish between graphs is limited to graphs separable  by the   Weisfeiler-Leman (WL) graph isomorphism test, and the strongest MPNNs, in terms of separation power, are those which are WL-equivalent. 

Recently, it was shown that the quality of separation provided by standard WL-equivalent MPNN can be very low, resulting in WL-separable graphs being mapped to very similar, hardly distinguishable features.

This paper addresses this issue by seeking bi-Lipschitz continuity guarantees for MPNNs. We demonstrate that, in contrast with standard summation-based MPNNs, which lack bi-Lipschitz properties, our proposed model provides a bi-Lipschitz graph embedding with respect to two standard graph metrics. Empirically, we show that our MPNN is competitive with standard MPNNs for several graph learning tasks, and is far more accurate on long-range tasks.
